{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  params:\n",
      "    batch_size: 2\n",
      "    num_workers: 8\n",
      "    train:\n",
      "      params:\n",
      "        size: 512\n",
      "        training_images_list_file: /home/santoshsanjeev/vqgan/train.txt\n",
      "      target: taming.data.custom.CustomTrain\n",
      "    validation:\n",
      "      params:\n",
      "        size: 512\n",
      "        test_images_list_file: /home/santoshsanjeev/vqgan/val.txt\n",
      "      target: taming.data.custom.CustomTest\n",
      "  target: main.DataModuleFromConfig\n",
      "model:\n",
      "  base_learning_rate: 4.5e-06\n",
      "  params:\n",
      "    ddconfig:\n",
      "      attn_resolutions:\n",
      "      - 24\n",
      "      ch: 128\n",
      "      ch_mult:\n",
      "      - 1\n",
      "      - 1\n",
      "      - 2\n",
      "      - 2\n",
      "      - 4\n",
      "      double_z: false\n",
      "      dropout: 0.0\n",
      "      in_channels: 3\n",
      "      num_res_blocks: 2\n",
      "      out_ch: 3\n",
      "      resolution: 512\n",
      "      z_channels: 256\n",
      "    embed_dim: 256\n",
      "    lossconfig:\n",
      "      params:\n",
      "        codebook_weight: 1.0\n",
      "        disc_conditional: false\n",
      "        disc_in_channels: 3\n",
      "        disc_start: 30001\n",
      "        disc_weight: 0.8\n",
      "      target: taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator\n",
      "    n_embed: 1024\n",
      "  target: taming.models.vqgan.VQModel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "config_path = \"./logs/2023-11-05T03-32-03_custom_vqgan/configs/2023-11-05T03-32-03-project.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "import yaml\n",
    "print(yaml.dump(OmegaConf.to_container(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 32, 32) = 262144 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n"
     ]
    }
   ],
   "source": [
    "from taming.models.vqgan import VQModel\n",
    "model = VQModel(**config.model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt_path = \"/share/ssddata/santosh_models/data_from_G42/temporal_project/final_data/CVPR/mimic_vqgan/last.ckpt\"\n",
    "sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "missing, unexpected = model.load_state_dict(sd, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santoshsanjeev/.conda/envs/taming/lib/python3.8/site-packages/torch/cuda/__init__.py:104: UserWarning: \n",
      "NVIDIA RTX A6000 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the NVIDIA RTX A6000 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fe287bca5b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda().eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import albumentations\n",
    "\n",
    "# Assuming your VQGanVAE class and required dependencies are defined in a module named vae\n",
    "from vae import VQGanVAE\n",
    "\n",
    "class YourTestClass:\n",
    "    def __init__(self):\n",
    "        self.indices_dict = {}  # Dictionary to store codebook indices\n",
    "        self.saved_model_path = None  # Path to save the model\n",
    "\n",
    "    # Function to load and preprocess the images\n",
    "    def load_images_from_txt(self, file_path):\n",
    "        # with open(file_path, 'r') as file:\n",
    "        #     lines = file.readlines()\n",
    "        return [file_path]\n",
    "\n",
    "    # Function to process images, obtain codebook indices, and save the model\n",
    "    def process_images_and_save_model(self, model_path, config_path, image_paths_file):\n",
    "        # Load image file paths from a txt file\n",
    "        file_paths = self.load_images_from_txt(image_paths_file)\n",
    "\n",
    "        # Initialize the VQGanVAE model\n",
    "        vqgan_model = VQGanVAE(vqgan_model_path=model_path, vqgan_config_path=config_path)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        vqgan_model.to(device)\n",
    "        vqgan_model.eval()\n",
    "\n",
    "        # Process each image and obtain codebook indices\n",
    "        for i, file_path in enumerate(file_paths):\n",
    "            try:\n",
    "                image = Image.open(file_path)#.convert('RGB')\n",
    "                if not image.mode == \"RGB\":\n",
    "                    image = image.convert(\"RGB\")\n",
    "                image = np.array(image).astype(np.uint8)\n",
    "                rescaler = albumentations.SmallestMaxSize(max_size = 512)\n",
    "                cropper = albumentations.CenterCrop(height=512,width=512)\n",
    "                preprocessor = albumentations.Compose([rescaler, cropper])\n",
    "\n",
    "                image = preprocessor(image = image)\n",
    "                image = (image['image']/127.5 - 1.0).astype(np.float32)\n",
    "\n",
    "                img_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "                with torch.no_grad():\n",
    "                    z_q, emb_loss, perplexity, indices = vqgan_model.get_codebook_indices(img_tensor)\n",
    "                # Store indices in the dictionary\n",
    "                # print(i, file_path, file_path.split('/')[-1][:-4])\n",
    "                self.indices_dict[file_path.split('/')[-1][:-4]] = indices.cpu().numpy().tolist()[0]\n",
    "                print(indices)\n",
    "                print(f\"Processed image {i + 1}/{len(file_paths)}\")\n",
    "                # decoded_image = vqgan_model.decode(indices)\n",
    "                # decoded_image = decoded_image.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()\n",
    "                # decoded_image = (decoded_image * 255).astype(np.uint8)\n",
    "                # decoded_image_save_path = f\"{'/l/users/santosh.sanjeev/logs/decoded_images'}/decoded_image_{i}.png\"\n",
    "                # Image.fromarray(decoded_image).save(decoded_image_save_path)\n",
    "                # print(f\"Decoded image saved: {decoded_image_save_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {i + 1}: {str(e)}\")\n",
    "        \n",
    "        # with open('', 'wb') as f:\n",
    "        #     pickle.dump(self.indices_dict, f)\n",
    "        # Save the VQGanVAE model after processing images\n",
    "        # torch.save(vqgan_model.state_dict(), save_model_path)\n",
    "        # print(f\"Model saved to {save_model_path}\")\n",
    "        # self.saved_model_path = save_model_path  # Store the saved model path for later use\n",
    "\n",
    "    # Function to load the saved model and decode images\n",
    "    def decode_and_save_images(self, codebook_indices_path, save_folder):\n",
    "        if self.saved_model_path is None:\n",
    "            print(\"No model path available. Please run 'process_images_and_save_model' first.\")\n",
    "            return\n",
    "        \n",
    "        # Load the saved model\n",
    "        vqgan_model = VQGanVAE()\n",
    "        vqgan_model.load_state_dict(torch.load(self.saved_model_path))\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        vqgan_model.to(device)\n",
    "        vqgan_model.eval()\n",
    "\n",
    "        # # Load codebook indices from the pickle file\n",
    "        # with open(codebook_indices_path, 'rb') as f:\n",
    "        #     self.indices_dict = pickle.load(f)\n",
    "\n",
    "        # Decode images and save them to the specified folder\n",
    "        for image_key, indices in self.indices_dict.items():\n",
    "            indices = torch.tensor(indices).to(device)\n",
    "            decoded_image = self.decode(indices, vqgan_model)\n",
    "            decoded_image = decoded_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "            decoded_image = (decoded_image * 255).astype(np.uint8)\n",
    "\n",
    "            image_save_path = f\"{save_folder}/{image_key}.png\"\n",
    "            Image.fromarray(decoded_image).save(image_save_path)\n",
    "            print(f\"Decoded image saved: {image_save_path}\")\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    test_obj = YourTestClass()\n",
    "    \n",
    "    # Replace these paths with your actual paths\n",
    "    model_path = '/share/ssddata/santosh_models/data_from_G42/temporal_project/final_data/CVPR/mimic_vqgan/last.ckpt'\n",
    "    config_path = '/share/ssddata/santosh_models/data_from_G42/temporal_project/final_data/CVPR/mimic_vqgan/2021-12-17T08-58-54-project.yaml'\n",
    "    image_paths_file = '/share/ssddata/physionet.org/files/mimic-cxr-jpg/2.0.0/files/p10/p10999737/s52341872/5aea5877-40b40fee-5bccd163-ca1bf0ce-a95c213d.jpg'\n",
    "    codebook_indices_path = './codebook_indices.pickle'\n",
    "\n",
    "\n",
    "    save_folder = './decoded_images'\n",
    "\n",
    "    # Process images and save the model\n",
    "    test_obj.process_images_and_save_model(model_path, config_path, image_paths_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unixgen_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
